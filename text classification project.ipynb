{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1) Creating the dictionary(vocabulary) from the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the os library to get the required folders \n",
    "import os\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#my_path is the variable to contain the destination\n",
    "my_path = r'20_newsgroups'\n",
    "folders = os.listdir(my_path) #this gives the list of all the folders present in the 20_newsgroups\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here sub_folders_path will contain the final locations of all the articles\n",
    "categories_path=[] #this will contain the location to all the folders\n",
    "j=0\n",
    "sub_folders_path=[]  #this will contain the final locations of all the folders\n",
    "for current_folder in folders:\n",
    "    categories_path.append(my_path + '\\\\' + current_folder) #appending the location\n",
    "    current_folder_sub_folders = os.listdir(categories_path[j]) #this gives the list of all the sub folders\n",
    "    subpaths = []\n",
    "    for k in current_folder_sub_folders:\n",
    "        subpaths.append(my_path + '\\\\' + current_folder + '\\\\'+ k)\n",
    "    sub_folders_path.append(subpaths)\n",
    "    j += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Naman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#for finding stop_words\n",
    "import nltk  #this libaray contains all the inbuilt stopwords\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english') #it gives a dictionary of all the stopwords\n",
    "stop_words={}\n",
    "for x in stopwords:\n",
    "    if x in stop_words:\n",
    "        stop_words[x]+=1\n",
    "    else:\n",
    "        stop_words[x]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#buildidng vocabulary\n",
    "vocabulary={}\n",
    "for i in range(0 , len(sub_folders_path)):\n",
    "    for j in range(0 , len(sub_folders_path[i])):\n",
    "        with open(sub_folders_path[i][j] , 'r') as f: #opening the file \n",
    "            data=f.read()\n",
    "            data=data.split() #creating the list of all the words present in the particualr file \n",
    "            for x in data:\n",
    "                if x not in stop_words:\n",
    "                    if x in vocabulary:\n",
    "                        vocabulary[x]+=1\n",
    "                    else:\n",
    "                        vocabulary[x]=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "463145"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorting the vocabulary into a list\n",
    "import operator\n",
    "sorted_vocab = sorted(vocabulary.items(), key=operator.itemgetter(1)) #I have sorted according to the increasing  occurrences of of words\n",
    "len(sorted_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461145"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is the count of words to keep\n",
    "k=len(sorted_vocab)-2000\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating final vocab with a range of 2000 words\n",
    "final_vocab={}\n",
    "while k<len(sorted_vocab):\n",
    "    final_vocab[sorted_vocab[k][0]]=sorted_vocab[k][1]\n",
    "    k+=1\n",
    "# now we have got our final vocab of 2000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_vocab)  #printing the len of final_vcab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2) a) creating features from the dictionary and putting it into a 2-d array for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the features in data for further training and testing\n",
    "X=[] # this will contain our final data on which we have to train and test\n",
    "for i in range(0 , len(sub_folders_path)):\n",
    "    for j in range(0 , len(sub_folders_path[i])):\n",
    "        with open(sub_folders_path[i][j] , 'r') as f:\n",
    "            \n",
    "            data=f.read()\n",
    "            data=data.split()   #all the words of the file have been created as a list of words \n",
    "            lst=[0 for i in range(2000)]  #since we are taking 2000 features\n",
    "            data_dict={}\n",
    "            for x in data:    #creating a dictionary of words since dictionaries are faster to access\n",
    "                if x in data_dict:\n",
    "                    data_dict[x]+=1\n",
    "                else:\n",
    "                    data_dict[x]=1\n",
    "                    \n",
    "            index=0          #writing down the count of words\n",
    "            for x in final_vocab:\n",
    "                if x in data_dict:\n",
    "                    lst[index]+=data_dict[x]\n",
    "                index+=1\n",
    "                \n",
    "            X.append(lst)   #appending the data point\n",
    "#so our final data in the form of a 2-d array is ready on which we have to work .            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2)b) building Y according to the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have to Build Y which will contain the target classes corresonding to x\n",
    "Y=[]\n",
    "index=0\n",
    "for i in range(0 , len(sub_folders_path)):\n",
    "    l=len(sub_folders_path[i])\n",
    "    for j in range(0 , l):\n",
    "        Y.append(folders[index])   #appending the corresponding classes \n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19997"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y) #this is the length of our Y which is the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the sklearn libarary\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train , X_test , Y_train , Y_test=train_test_split(X , Y , random_state=1)  #splitting the data (random_state is to ensure we get similar kind of data everytime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3) Now train and test data on inbuilt Multinomial NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the required things from sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report , confusion_matrix , accuracy_score  #thises parameters will help us compare later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=MultinomialNB() #creating an algo of MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train , Y_train) #fitting the training data on algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred=clf.predict(X_test) #predicting the values on X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inbuilt_training_score=clf.score(X_train ,Y_train) # checking the accuracy on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inbuilt_testing_score=clf.score(X_test , Y_test)   #checking the accuaracy on testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4) Implementing self built Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4)a) creating the fit function for X_train and Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_data(X_train , Y_train):\n",
    "    different_classes=set(Y_train)  #this will give the different classes\n",
    "    result={} \n",
    "    for current_class in different_classes:\n",
    "        result[\"total data\"]=len(Y_train)  #this gives the total number of rows\n",
    "        result[current_class]={}    #this dictionary will contain the \n",
    "        current_class_rows=(Y_train==current_class)   #this will generate the required number of rows of a particular class\n",
    "        X_train_new=X_train[current_class_rows]\n",
    "        Y_train_new=Y_train[current_class_rows]\n",
    "        result[current_class][\"total count\"]=len(Y_train_new)\n",
    "        num_features=X_train_new.shape[1]\n",
    "        for i in range(0 , num_features):\n",
    "            result[current_class][i]=X_train_new[: , i].sum()   #keeping a count of all the words in a particular class\n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4)b) creating the probabilty function to check the probability of a data point belonging to a particular class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(dictionary , current_class , x):\n",
    "    \n",
    "    total_classes=dictionary[\"total data\"]   #this is the number of rows in the dataset. It contains all classes\n",
    "    \n",
    "    output=np.log(dictionary[current_class][\"total count\"])-np.log(total_classes)  #using log so as to avoid getting too small numbers\n",
    "    \n",
    "    num_features=len(dictionary[current_class].keys())-1   #this will give the number of features\n",
    "    \n",
    "    total_words_in_current_class=0\n",
    "    for y in dictionary[current_class]:\n",
    "        if y==\"total count\":\n",
    "            continue\n",
    "            \n",
    "        total_words_in_current_class+=dictionary[current_class][y]   #this give the sum of all words\n",
    "        \n",
    "    total_words_in_current_class+=num_features\n",
    "            \n",
    "        \n",
    "    for i in range(0 , num_features):\n",
    "        if x[i]>0:       #this condition is to check whther the word exists in the vocabulary or not\n",
    "            count_of_word=dictionary[current_class][i]+1\n",
    "            word_probability=np.log(count_of_word)-np.log(total_words_in_current_class)\n",
    "            output+=word_probability\n",
    "        \n",
    "    return output        # this is the final probabilty of an article belonging to a particular class\n",
    "         \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step4)c) getting the best class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(dictionary , x):\n",
    "    classes=dictionary.keys()   #this give us all the distinct classes \n",
    "    best_prob=-1000\n",
    "    best_class=\"\"\n",
    "    first_run=True     #this is to ensure the assignment of a value to best_prob_and best_class during the first run\n",
    "    for current_class in classes:\n",
    "        if current_class==\"total data\":\n",
    "            continue\n",
    "        p_current_class=probability(dictionary , current_class , x)  #calling the probabilty function to calculate the probability of each class\n",
    "        if first_run or p_current_class>best_prob:\n",
    "            best_prob=p_current_class\n",
    "            best_class=current_class\n",
    "        first_run=False\n",
    "    return best_class   #return the best class with the highest probability for an article belonging to a particular class\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step4)d) predicting the final values for testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_output(dictionary , X_new_test):\n",
    "    Y_pred=[]  #this will contain our final predictions for testing data\n",
    "    for x in X_new_test:\n",
    "        ans=predict_class(dictionary , x)  #calling the function predict_class to get the required class\n",
    "        Y_pred.append(ans)\n",
    "    return Y_pred  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the numpy library foir mathematical calculations\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the previous arrays into numpy arrays for easier calculations\n",
    "X_new=np.array(X)\n",
    "Y_new=np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data (random state is used to get the similar type of data each time we run the program)\n",
    "X_new_train , X_new_test , Y_new_train , Y_new_test=train_test_split(X_new , Y_new , random_state=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the fit function to return the dictionary containing all the info\n",
    "dictionary=fit_data(X_new_train , Y_new_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calling the predict_output function to get final output\n",
    "Y_new_pred=predict_output(dictionary , X_new_test)\n",
    "self_test_score=accuracy_score(Y_new_pred , Y_new_test) #calculating the accuarcy of our predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step5) Comparison between the inbuilt multinomial naive bayes and self implemented naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of inbulit multinomial naive bayes on testing data is 0.7888\n",
      "The accuracy of self implemented multinomial naive bayes on testing data is  0.8094\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy of inbulit multinomial naive bayes on testing data is\" , inbuilt_testing_score)\n",
    "print(\"The accuracy of self implemented multinomial naive bayes on testing data is \" , self_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
